<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>




<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="verify-v1" content="0c/Cx6NLE4aEaOKo//DNIIJVpWqeq374fNJIL//A2wE=">
<title>Duke Listens!</title>
           <!-- a reliable way to detect ie in javascript -->
<script type="text/javascript">lteie6=lteie7=false;</script>
<!--[if lte IE 6]><script type="text/javascript">lteie6=true;</script><![endif]-->
<!--[if lte IE 7]><script type="text/javascript">lteie7=true;</script><![endif]-->


        

    
    
<script type="text/javascript" src="support_files/dom-utils.js"></script>
<script type="text/javascript" src="support_files/custom.js"></script>
<link rel="stylesheet" type="text/css" href="support_files/rainforest-custom.css">
    </head><body class="weblog">
<!-- End of StatCounter Code -->

<div id="pagewrap">
<div id="innerpagewrap">

<div id="header">
<div id="innerheader">
<h1 class="has-subhead"><a href="page0.html">Duke Listens!</a><span class="colon">:</span> <span class="subhead">Visit my main blog at <a href="http://musicmachinery.com/">MusicMachinery.com</a></span>  </h1>
<p class="skip">Skip to <a href="page7.html#content">content</a>, <a href="page7.html#nav">navigation</a>.</p>
</div><!-- end #innerheader -->
</div><!-- end #header -->

<div id="content">
<div id="innercontent">


                <div class="day">

	<div class="day-title">
	<h2>Thursday Sep 18, 2008</h2>
	</div>

	<div class="entry" id="ilike_launches_developer_platform">

	<div class="entry-title entry-info">
			<h3><a href="ilike_launches_developer_platform.html" title="permalink">iLike launches developer platform</a></h3>
		</div>

	<div class="entry-content">
				More info <a href="http://blog.ilike.com/ilike_team_blog/2008/09/ilike-launches.html">here</a> and <a href="http://www.ilike.com/developer/">here</a>.
		</div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 18, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ilike_launches_developer_platform.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="how_to_evaluate_a_playlist">

	<div class="entry-title entry-info">
			<h3><a href="how_to_evaluate_a_playlist.html" title="permalink">How to evaluate a playlist?</a></h3>
		</div>

	<div class="entry-content">
				<a href="https://flickr.com/photos/julishannon/2449735973/"><img src="support_files/2449735973_deca895617_m.jpg" style="float: right; margin: 10px;"></a>I
 have a number of conversations with researchers about playlist 
generation and how best to evaluate how good a playlist is. It is not an
 easy problem, some are skeptical about whether it can be done at all 
without thousands and thousands of real music listeners as evaluators - 
but lots of people have ideas.  A number of researchers seem keen on 
figuring out how best to evaluate playlists in a more formal setting 
like MIREX. 
<p>
Ben Fields has started the ball rolling with a post on his blog: <a href="https://stuffalsothings.blogspot.com/2008/09/introductory-thoughts-on-playlist.html">introductory thoughts on a playlist generation task in MIREX 2009</a>.  If you are interested in playlist generation, then join the conversation.
</p><p>
Some ideas that have been floated for a playlist evaluation:
</p><ul>
<li> The traditional IR approach - use a large database of human 
generated playlists (from webjay, musicmobs etc), randomly remove tracks
 from the playlist - calculate precision and recall for systems that try
 to predict what tracks were removed.
</li><li> Human evaluation -  Have experts (DJs, music critics,) and non-experts evaluate the playlists.
</li><li> Create a reverse turing test - present each system with a set 
of playlists - some human created, some created at random - systems try 
to predict which playlists are human generated.
</li></ul>
Any more ideas? 


		</div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 18, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="how_to_evaluate_a_playlist.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_3_mostly_mirex">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_3_mostly_mirex.html" title="permalink">ISMIR Day 3 - Mostly MIREX Posters</a></h3>
		</div>

	<div class="entry-content">
				
<h2> MIREX Posters</h2>
The poster session on Day 3 was dominated by the MIREX submissions.  I'm particularly interested in this year's new task on <a href="http://www.music-ir.org/mirex/2008/index.php/Audio_Tag_Classification">autotagging </a> which had a number of good submissions.
<p>
The autotagging submission by K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vlahava </p><p> 
<img src="support_files/_IMG_0660.JPG" alt="IMG_0660.JPG" border="0" width="600">
</p><p>

</p><p>
Mike Mandel describes LabRosa's autotagger.
</p><p>
<img src="support_files/_IMG_0662.JPG" alt="IMG_0662.JPG" border="0" width="600">
</p><p>
Kris West describes the MIREX 2008 audio classificatiom task.</p><p>
<img src="support_files/_IMG_0663.JPG" alt="IMG_0663.JPG" border="0" width="600">
</p><p>
</p><p>
Andreas Ehmann shows off the MIREX summary poster.  Look at all those numbers.
</p><p>
<img src="support_files/_IMG_0664.JPG" alt="IMG_0664.JPG" border="0" width="600">
</p><p>

Luke Barrington and Doug Turnbull present their autotagger.
</p><p>
<img src="support_files/_IMG_0665.JPG" alt="IMG_0665.JPG" border="0" width="600">
</p><p>
George Tzanetakis and Malcolm Slaney have a chat.
</p><p>
<img src="support_files/_IMG_0669.JPG" alt="IMG_0669.JPG" border="0" width="600">
</p><p>
Geoffrey Peeters presents his generic training and classification system for MIREX.
</p><p>

<img src="support_files/_IMG_0670.JPG" alt="IMG_0670.JPG" border="0" width="600">
</p><p>

</p><p>
Doug describes the size of the brain needed to understand his work on the <a href="http://ismir2008.ismir.net/papers/ISMIR2008_261.pdf">use of sparse time-relative auditory codes for music</a>.
</p><p>
<img src="support_files/_IMG_0652.JPG" alt="IMG_0652.JPG" border="0" width="600">
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 18, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_3_mostly_mirex.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>

</div>
            <div class="day">

	<div class="day-title">
	<h2>Wednesday Sep 17, 2008</h2>
	</div>

	<div class="entry" id="ismir_day_3_mir_methods">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_3_mir_methods.html" title="permalink">ISMIR Day 3 - MIR Methods and Platforms (Sporking the Shreds)</a></h3>
		</div>

	<div class="entry-content">
				<h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_110.pdf">A Tunneling-Vantage Indexing Method for Non-Metrics
</a></h2>Rainer Typke and Agatha Walczak-Typke
<p>
Ranier and Agatha are looking at creating an instance of the Earth 
Mover's Distance that can be used to compare rhythmical patterns.  hey 
use a unique way of pre-calcuating tunnels between the subspaces that 
link nearest  in the different subspaces.

Evaluation:  40,000 sequences with random errors. With large search 
radiuses they get decent precisions (they get perfect recall).

"It's always a good idea to closely inspect your balls - their shape 
might allow you to apply optimum vantage indexing to your favorite 
metric, resulting in retrieval in O(Log n) without any false positives 
or false negatives.

</p><p>
<img src="support_files/_ranier.png" alt="ranier.png" border="0" width="600">

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_253.pdf">The Perlhumdrum and Perllilypond Toolkits for Symbolic Music Information Retrieval</a></h2>
Ian Knopke
<p>
An alternative toolkit for working with large collections of Humdrum 
files.  Works with Lilypond.  Humdrum is designed for the analysis of 
music in symbolic form. Humdrum is getting stale - many of the tools run
 in awk (Woohoo!) that just don't work anymore with the latest awk.  So 
Ian started rewriting them all in Perl. Eventually he decided that there
 might be a better way to go instead of just rewriting humdrum piece by 
piece. Ian walked us through some humdrum notation, and the new 
PerlHumdrum, and PerlLilypond toolkits. Eleanor Selfridge asked some 
questions about how people should use PerlHundrum - should they use the 
scripts or some higher level packaging.

</p><p>
<img src="support_files/_humdrum.png" alt="humdrum.png" border="0" width="600">
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_151.pdf">Support for MIR Prototyping and Real-Time Applications in the ChucK Programming Language</a></h2>
Rebecca Fiebrink, Ge Wang and Perry Cook 
<p>
Rebecca and Ge talk about using ChucK for MIR prototyping.  First Ge 
gave the quick overview of ChucK (this was a very quick summary of what 
we learned in the ChucK tutorial).  Rebecca went on to talk about ChucK 
in the performance context. One unique things about ChucK is that you 
can write code in a performance context.  Ge gave a demo with the 
MiniAudicle. Ge once again was writing code during a talk.  

They moved onto showing how ChucK can be used for analysis (with the 
aptly named 'upchuck' operator).
Implications for MIR: feature extraction, algorithms, with on the fly 
modifications to let you try things quickly, simple concurrency.  Easy 
to teach and learn, hands-on, immediate feedback - real-time MIR, 
specially in live performance.

Weka-style API analysis - you can do classification an run it on the 
fly.  You could even train a classifier during a performance (so not 
just perfomrance coding, but performance machine learning - woah).
</p><p>
Rebecca gave a great demo showing live feature extraction, extracting 
marsyas-like features,  along with an adaboost classifier.  Rebecca did 
live training - classical piano for one class, and orchestral for 
another class.  The classifier worked really well.
</p><p>
<img src="support_files/_IMG_0655.JPG" alt="IMG_0655.JPG" border="0" width="600"> </p><p><i> Rebecca is the Keith Emerson of PlorK (although she doesn't know who Keith Emerson is)</i>
</p><p>
Hopes: - want to see more MIR work done in real-time contexts. Think that this may be a different way to think and do things.
</p><p>
TakeOut - chuck is unique, analysis and synthesis in the same framework,
 strongly timed, on the fly is great for rapid prototyping.
</p><p> SMIRK - smirk.cs.princeton.edu

</p><p>
<img src="support_files/_chuck.png" alt="chuck.png" border="0" height="470" width="453">
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_3_mir_methods.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="mining_the_myspace_social_graph">

	<div class="entry-title entry-info">
			<h3><a href="mining_the_myspace_social_graph.html" title="permalink"> Mining the MySpace social graph</a></h3>
		</div>

	<div class="entry-content">
				<a href="http://support_files/_IMG_0646.JPG"><img src="support_files/_IMG_0646.JPG" alt="IMG_0646.JPG" style="float: left; margin: 10px;" border="0" width="100"></a>One really cool source of music data is this <a href="http://dbtune.org/myspace/">live RDF representation of Myspace users</a>.
  This RDF representation gives you a FOAF-style view of a myspace user.
  If the user happens to be a music artist, the corresponding tracks for
 the artist included in the RDF.  This gives you a nice way of 
extracting the artist social graph at MySpace - which can be used to do 
all sorts of things from artist similarity to playlist generation.  The 
code is <a href="https://motools.svn.sourceforge.net/viewvc/mypyspace/musicGrabber/branches/webserv-branch/">open sourced</a>.<p><em>Update</em> <a href="http://blog.dbtune.org/">Yves</a> and <a href="https://kurtisrandom.blogspot.com/">Kurt</a>
  are responsible for the RDF translation service with some recent input
 from Mischa Tuffield of Garlik (to help to refine the service and the 
supporting ontology).   <a href="https://stuffalsothings.blogspot.com/">Ben Fields</a> did loads of work gathering the data set and extracting audio features. </p><p>
Keep an eye on these guys, they are going places!


		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="mining_the_myspace_social_graph.html">Permanent link to this entry</a>
																| <a href="https://kiserai.net/DukeListens/mining_the_myspace_social_graph#comments.html">Comments [2]</a>
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_2008_mirex_panel">

	<div class="entry-title entry-info">
			<h3><a href="ismir_2008_mirex_panel.html" title="permalink">ISMIR 2008 Mirex panel</a></h3>
		</div>

	<div class="entry-content">
				Stephen Downie gave an overview of MIREX and described the special 
challenges and issues for this year. He then opened it up to a general 
discussion. Some comments
<ul>
<li> Doug Eck:  Make data available, make it easier for machine learning types to participate
</li><li> Brian, Daniel M. - can we/should we use more social data in the evaluations
</li><li> Some discussion about using virtualization technology to allow the participants to login and run their submissions.
</li><li>Geoffry - repeats the suggestion for making subsets of data 
available.  Perhaps people who participate should have to provide some 
data as well.
</li><li> Scheduling - Kris suggests that perhaps we should decouple MIREX from ISMIR.
</li></ul>
An interesting discussion - but folks seem to be thinking that MIREX 
really should be a source of data for researchers - when the role of 
MIREX is really for evaluations.  Anyone who wants to share data should 
think about setting up a MIRDX (MIR Data eXchange).
<p><img src="support_files/_IMG_0659.JPG" alt="IMG_0659.JPG" border="0" width="600">
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_2008_mirex_panel.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_3_content_based">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_3_content_based.html" title="permalink">ISMIR Day 3 - Content based similarity and retrieval</a></h3>
		</div>

	<div class="entry-content">
				<h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_209.pdf">Social
 Playlists and Bottleneck Measurements : Exploiting Musician Social 
Graphs Using Content-Based Dissimilarity and Pairwise Maximum Flow 
Values</a></h2>
Ben Fields, Kurt Jacobson, Christophe Rhodes and Michael Casey
<p>
Ben Field (dressed to kill) talks about his interesting work in looking at acoustic similarity and social networks. 

<strong>Motivation</strong>: There's a glass ceiling for content-based classification. <strong>Novelty</strong>:
 Don't always want 'similar' music. Novelty is important.  
Social/cultural awareness .  Can we improve recommendations and playlist
 by incorporating social context and not just acoustic similarity.
</p><p>
Ben conducted a flow analysis of myspace artists, looking for nodes.  
How: Start with a randomly selected myspace artist, grab their 'top 
friends' (near neighbors in the graph) and repeat.  Do this 6 times - 
yields &gt; 15,000 artist and 60K tracks.  120K directed edges - 15.5 
edges per node. Ben showed that this graph follows the Duncan watts 
power law.  
</p><p>
So what do you do? - group artists based on maximum flow and compared 
with acoustic similarity. Not too surprisingly, these spaces are not 
well correlated. (These plots compare the Earth Mover's distance, vs. 
maximum flow value).
</p><p>
What can we do with this? - Look at meaningful ways to exploit this 
difference in acoustic and graph similarity.  A playlist generator could
 be created by following maxflow paths through a social graph, where the
 cost of the paths are determined by acoustic similarity. This is a neat
 idea.
</p><p>
<img src="support_files/_fields.png" alt="fields.png" border="0" height="816" width="456">
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_181.pdf">Music Genre Classification: A Multilinear Approach</a></h2>
Ioannis Panagakis, Emmanouil Benetos and Constantine Kotropoulos
<p>
This team is looking at automatic genre classification using a 
bio-inspired cortical representation of the slow multi-scale spectro- 
temporal modulations of each recording is used.  The author described in
 great detail their novel method for approaching genre classification 
using cortical Representation with wavelet analysis and tensors. Most of
 what he talked about was beyond me. He does seem to be bringing a new 
toolkit from bioinformatics to apply to music, with interesting results.
  This talk has the most equations per slide of ISMIR so far (50 times 
more equations then in Malcolm's talk).
</p><p>
<strong>Results</strong>: about 80% accuracy - which corresponds to at or near the state-of-the-art for genre classification.  
</p><p>
<img src="support_files/_ionnis.png" alt="ionnis.png" border="0" width="600">
</p><p>

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_259.pdf">Combining Features Extracted From Audio, Symbolic and Cultural Sources</a></h2>
Cory McKay and Ichiro Fujinaga
<p>
<img src="support_files/_IMG_0650.JPG" alt="IMG_0650.JPG" style="float: left; margin: 10px;" border="0" width="200">
</p><p>
Cory describes their investigations of the classification utility of 
combining features extracted from separate audio, symbolic and cultural 
sources of musical information.  Again, this is a genre classification 
experiment.  
</p><p>
Can you improve classification performance by combining features. Also 
cory looks at the types of classification errors.  Will this help us 
break through the glass ceiling?
</p><p>
They extracted the 3 types of features and compared classification of 
all 7 subsets. They created a new dataset: the SAC Dataset - Symbolic  
Audio Cultural.
SAC has 10 genres of music, that can be collapsed into 5 genres. 
Use the <a href="http://jmir.sourceforge.net/">jMIR</a> toolkit for analysis.
</p><p>
<strong>Results</strong>: When combining all 3 features, classification 
accuracy improved.  However the improvements of 3 types over 2 is not 
statistically significant. Looking at misclassifications - they used a 
misclassification cost function that reduces the penalty for 
misclassifying to a similar genre. When using these weighted rates and 
cultural features are used, the misclassifications tend to be in related
 genres.  Results for a 10 genre taxonomy: 78.8% vs. the 68% results 
from last year - indicating that combining the feature types helps break
 through the glass ceiling.
Next: The want to use a larger dataset with a larger class ontology.
</p><p> Montreal Genre Classification Smackdown - Doug Eck doubts that 
the symbolic features are really necessary, but Cory thinks that they 
are.   
</p><p> This was a very clear presentation. Well done, Cory.
</p><p>
<img src="support_files/_cory.png" alt="cory.png" border="0" height="858" width="482"> 
</p><p>

		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_3_content_based.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_3_keynote_noting">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_3_keynote_noting.html" title="permalink">ISMIR Day 3 - Keynote - Noting time</a></h3>
		</div>

	<div class="entry-content">
				The ISMIR keynote is being delivered by Jeanne Bamberger the 
Professor of Music emerita at the Massachusetts Institute of Technology 
where she teaches music theory and music cognition. She is also 
currently Visiting Professor of Education at UC-Berkeley.  Her research 
is interdisciplinary: integrating music theory and practice, modes of 
representation, and recent approaches to cognitive development, she 
focuses on close analysis of children and adults in moments of 
spontaneous learning.
<p>
I think the best strategy for a keynote, is just to listen ... so I won't be taking any notes.
</p><p> <i>Update</i> Doug Eck poked Jeanne into telling us what we in 
the MIR community is doing wrong.  She says - what we are doing is not 
going to help us answer some fundamental and simple questions about 
music. 
</p><p>
Jeremy asks a question about where the boundary between transcription 
and notation is (isn't a recording just a very fine grained notation?) 
This gives Jeanne a stomach-ache.
</p><p>
Jeanne has a number of books <a href="https://www.amazon.com/s?ie=UTF8&amp;search-type=ss&amp;index=books&amp;field-author=Jeanne%20Bamberger&amp;page=1">available at amazon</a>.
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_3_keynote_noting.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_2_posters">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_2_posters.html" title="permalink">ISMIR Day 2 Posters</a></h3>
		</div>

	<div class="entry-content">
				Another day at ISMIR - another set of interesting posters.  Again, 
I'm  highlighting posters that are relevant to our work here at Sun 
Labs.

<h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_117.pdf">Music,
 Movies and Meaning: Communication in Film-Makers’ Search for 
Pre-Existing Music, and the Implications for Music Information Retrieval</a></h2>
Charlie Inskip, Andy Macfarlane and Pauline Rafferty 
<img src="support_files/_IMG_0635.JPG" alt="IMG_0635.JPG" border="0" width="600">
<p>
Charlie brings lots of insight into the problems of finding music for 
film - the clash of cultures and the different (and sometimes 
non-overlapping) vocabularies get in the way.  (Charlie is also a really
 interesting guy to talk to ... he really knows a lot about popular 
music)

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_133.pdf">Hit Song Science is Not Yet a Science</a></h2>
Francois Pachet and Pierre Roy
<p>
Pierre presenting his poster debunking Hit Song Science. (As Jim Waldo 
says ... anything with 'science' in its name probably isn't)
</p><p>
<img src="support_files/_IMG_0626.JPG" alt="IMG_0626.JPG" border="0" width="600">
</p><p>

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_266.pdf">Hubs and Homogeneity: Improving Content-Based Music Modeling</a></h2>
<a href="http://www.music.gatech.edu/mtg/students/mark_godfrey/">Mark</a> Godfrey and <a href="http://paragchordia.com/">Parag</a> Chordia 
<img src="support_files/_IMG_0628.JPG" alt="IMG_0628.JPG" border="0" width="600">
<p>
Mark and Parag were describing some their work to avoid hubs that occur with some timbre-based similarity models.

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_130.pdf">Content-Based Musical Similarity Computation Using the Hierarchical Dirichlet Process</a></h2>
Matthew Hoffman, David Blei and Perry Cook 
<img src="support_files/_IMG_0630.JPG" alt="IMG_0630.JPG" border="0" width="600">
<p> Matthew showing his promising work that is faster and potentially and higher quality than classical approaches.

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_235.pdf">Rhyme and Style Features for Musical Genre Categorisation By Song Lyrics</a></h2>
Rudolf Mayer, Robert Neumayer and Andreas Rauber
<img src="support_files/_IMG_0632.JPG" alt="IMG_0632.JPG" border="0" width="600"><p>
Some interesting and fun work looking at features in lyrics that can be used for music classification.

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_263.pdf">The 2007 MIREX Audio Mood Classification Task: Lessons Learned</a></h2>
Xiao Hu, J. Stephen Downie, Cyril Laurier, Mert Bay and Andreas F. Ehmann 

<img src="support_files/_IMG_0640.JPG" alt="IMG_0640.JPG" border="0" width="600"><p>
The MIREX team showing detailed results from the last year's Audio Mood 
classification task.  It is really interesting to see the amount of 
improvement that occurred from 2006 to 2007. Apparently the same 
improvement occurred this year.  Cyril is doing some really interesting 
things with mood.  He showed me his mood music player that shows a 
real-time indicator of the mood of the currently playing song as well as
 a really nice music search engine being developed at <a href="http://www.bmat.com/">BMAT</a> that allows you to query for music based upon multiple moods and other aspects of the music.

		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 17, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_2_posters.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>

</div>
            <div class="day">

	<div class="day-title">
	<h2>Tuesday Sep 16, 2008</h2>
	</div>

	<div class="entry" id="ismir_day_2_session_2">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_2_session_2.html" title="permalink">ISMIR Day 2 Session 2 - Performance Analysis and Music Summarization</a></h3>
		</div>

	<div class="entry-content">
				<h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_210.pdf">Using Expressive Trends for Identifying Violin Performers</a></h2>
Miguel Molina-Solana, Josep Lluis Arcos and Emilia Gomez
<p>
Miguel presents the goal - identify violinists by their playing style 
using commercial audio recordings and state-of-the-art feature 
extraction tools.
</p><p>
<strong>Problems</strong> : High heterogenicity, Partial Accuracy, Perform variability.
</p><p><strong>Approach</strong>:  characterize performers by analyzing 
only the feature trends.Three steps: Feature extraction,  Modeling 
trends,  Classify models
</p><p>
<strong>Evaluation</strong>: 23 different violinists. This graph shows the correct performer position for each of the 23 violinist.
</p><p>
<img src="support_files/_solana.png" alt="solana.png" border="0" height="420" width="473">
</p><p>
They achieved good results with just a small amount of data.

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_240.pdf">Hybrid Numeric/Rank Similarity Metrics for Musical Performance Analysis</a></h2>
Craig Sapp
<p>
This paper describes a numerical method for examining similarities among
 tempo and loudness features extracted from recordings of the same 
musical work and compares its performance to a Pearson correlation.
</p><p>
89 performances of mazurka - more info here: <a href="http://www.charm.rhul.ac.uk/content/projects/chopin.html">CHARM</a> and <a href="http://www.mazurka.org.uk/">the Mazurka Project</a>
</p><p>
Evaluation - how well can 2 of Rubinstein's  performances be identified 
with the 3rd. Craig really takes advantage of visualizations in his work
 - I find it to be incredibly interesting (although I don't really 
understand it all very well).
</p><p>
<img src="support_files/_sapp2.png" alt="sapp2.png" border="0" height="402" width="500">
<img src="support_files/_sapp.png" alt="sapp.1.png" border="0" height="519" width="482">
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_233.pdf">Creating and Evaluating Multi-Phrase Music Summaries</a></h2>
Konstantinos Meintanis and Frank Shipman 
<p>
This paper explores whether the composition of multiple characteristic 
phrases that are selected to be highly dissimilar to one another will 
increase the summary's effectiveness.</p><p>
Goal - select the most salient phrase from a song to serve as a music 
summary - useful, for instance, in an online music store - or for 
personal organization. The summary should be short and recognizable.  
The author suggests that one highly repeated phrase does not guarantee 
better effectiveness.  Highly repeated, yet different from one another, 
or distinct (non-repeated) phrases.

</p><ul>
<li> REA - emphasizes repeated  phrases
</li><li>SDEA - emphasizes phrases that are sonically distinct
</li><li>IA = Intermediate algorithm
</li></ul>
Results - 90% of surveyed said that selected summary was at a good 
choice. Much better than just using the intro for the song.  The REA 
algorithm works best. The short demo was really interesting (I like the 
Gary Moore track he played too).
<p>
<img src="support_files/_summary.png" alt="summary.png" border="0" height="519" width="464">
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 16, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_2_session_2.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_2_session_1">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_2_session_1.html" title="permalink">ISMIR Day 2 Session 1 - Plenary Session 3: Content-based Similarity &amp; Retrieval 1</a></h3>
		</div>

	<div class="entry-content">
				<h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_133.pdf">Hit Song Science is Not Yet a Science</a></h2>
Francois Pachet and Pierre Roy 
<p>
The researchers look at a large set of data (32000 titles, with 632 labels per title:
</p><ul>
<li>acoustic - genre, epoch, instrumentation 
</li><li>facts - langauge, subjective, popularity
</li><li>subjective - campfire, farewll
</li></ul>
<p>
<img src="support_files/_hss.png" alt="hss.1.1.png" border="0" height="276" width="425">
</p><p>
The used MIR techniques to see if the could learn any of these labels from acoustic features. 
</p><ul>
<li>Grounding and semantics of labels are not related - labes 
representing acoustic properties are not always easy to learn using 
acoustic features, but some highly subjective labels can be well modeled
 by acoustic classifiers.
</li><li>they cannot predict hits  - does a likeliness of a hit depend only on the quality of the song.  Quotes / Summarizes the <a href="https://www.sciencemag.org/cgi/content/abstract/311/5762/854">Duncan Watts study</a> that indicate that social effects are extremely important.
</li></ul>
The researchers report that after using many different techniques 
they've concluded that, acoustically - there's no correlation between 
the features and the popularity.
<p>
</p><h2>	<a href="http://ismir2008.ismir.net/papers/ISMIR2008_130.pdf">Content-Based Musical Similarity Computation Using the Hierarchical Dirichlet Process</a></h2> by Matthew Hoffman, David Blei and Perry Cook 
<p>
<img src="support_files/_130.png" alt="130.png" border="0" height="371" width="396">
</p><p>
Timbral similarity - motivations: Query, playlists, recommendation.  
Matt started off with explaining the classic approaches: mixtures of 
gaussians, kl-divergence.  HDP Details - Dirichlet Process Mixture Model
 (DPMM). Matt uses the chinese restaurant metaphor effectively to 
explaining the DPMM.
</p><p>
<img src="support_files/_chinese.png" alt="chinese.png" border="0" height="588" width="376">
</p><p>
Evaluation:  Use the SXSW artist.  4 models, single guassian, 
k-component GMMs, VQ Codebook, and HDP. Use genre as a proxy for 
similarity.  Results show that HDP shows an improvement over the other 
techniques.  HDP and the VQ methods did not produce many hubs or anti 
hubs (songs that are wrongly evaluated as similar or dissimilar to all 
other songs). And it is faster.  Good paper - and very solid looking 
work, although the data set is quite small.
</p><p>
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_148.pdf">Learning a Metric for Music Similarity</a></h2>
Malcolm Slaney, Kilian Weinberger and William White 
<p>
Malcolm presents work at Yahoo.  Goal: Describe techniques to build a 
metric space for better classification or similarity of songs.

Euclidean metrics - scale matters a lot.independece.   Malcolm suggests 
applying a rotation and scaling matrix to features to deal with these 
issues of scale and independence. Where does the matrix come from?

</p><ul>
<li>Whiten - normalize the data so all features have same variance. 
</li><li> LDA - maximize the inter vs. intra class distance.
</li><li> RCA - equalize class dimension
</li><li> NCS - Neighborhood component analysis 
</li></ul>
<p>
Optimize the classification error - but this can be problematic with KNN
 because of the boundary problem. The Large Margin nearest neighbor can 
help this.
</p><p>
<img src="support_files/_slaney.png" alt="slaney.png" border="0" width="600">
</p><p>
Slaney encourages us to use these simple methods to improve our similairty metrics.
 
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 16, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_2_session_1.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_2_the_music">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_2_the_music.html" title="permalink">ISMIR Day 2 - the music recommendation panel</a></h3>
		</div>

	<div class="entry-content">
				<h2>Commercial Music Discovery and Recommendation 
</h2><b>Moderator</b>: Youngmoo

  <p> <img alt="IMG_0621.JPG" src="support_files/_IMG_0621.JPG" border="0" width="600"> </p>
  <h2>Panelists:
</h2> 
  <ul> 
    <li>Markus Cremer - Gracenote
</li>
    <li>Etienne Handman - Pandora
</li>
    <li>Elias Pampalk - Last.fm
</li>
    <li>Anthony Volodkin - The Hype Machine
</li>
    <li> Brian Whitman - Echonest
</li>
  </ul>
ISMIR day 2 starts off with a panel on a topic that is near and dear to my heart:  Music Discovery and Recommendation.

  <p>
Markus gave a history of Gracenote - starting with CDDB. There drive to 
go commercial stemmed from the need to clean up all the noisy, messy 
data. Next Markus showed the 'Gracenote solution' slide that highlights 
the various Gracenote products.  Markus finally got to the 
recommendation at the end of his time - 
</p>
  <p>
Etienne - describes the music genome project - classification leading to
 recommendation. They use mostly human analysis but do use a little 
computer analysis (note to self, ask Etienne what they do) 1,000<strike>0</strike>
 genes, 200 or 300 apply to a given piece, 15,000 new analysis, "we use 
people". Originally a b2b play - but now focus on the radio experience -
 (which is different from recommendation) - serendipity.  They are proud
 of the playlisting - listener feedback is very important (over a 
billion are used to assist in playlisting).  Etienne also ran out of 
time before he could finish his pitch.
</p>
  <p>
Elias introduced last.fm - with a succinct description of last.fm.  
Human-to-human recommendation is very important.  Lean forward and lean 
back modes.   Next <i> (update) 5</i> years: 200 million tracks. 
subgenres will grow much faster. More data will lead to better 
recommendations, better response to new trends etc.  Data Portability, 
artists will embrace recommendations (watch out for shilling).  Elias 
finished his presentation time to spare.
</p>
  <p>
Anthony - gives an overview of hype machine.  1.2 million unique 
visitors (40% us, 12% uk, 6% germany).  Why do so many people like HM?  
 Scratching Anthony's personal itch - find new music - he wanted find 
music that real people were excited about. Didn't like marketing - but 
liked blogs - blogs are "the excitement filter" - this is the discovery 
experience - the music that people on the web are excited about. What's 
next?  - Similar  / related music recommendations will become a 
commodity (good point, this is already happy) - it is all going to be 
about user experience (UI and context will differentiate).  Transparency
 is important.  Interfaces are important.
Points to <a href="http://www.thesixtyone.com/">thesixtyone.com</a> as an example of how to make music fun.  A great 4 minute intro (he's a pro!)
</p>
  <p>
Brian talks about the EchoNest - the goal is to sell MIR to people - 
they have built lots of tech that they use people for free.  Everything 
should be done by machines, no editors, no filters.  They've made all of
 their data available via their web services. They really want people in
 the world to use their services.  Points to MoreCowbell.dj a fun app.
Another demo - they've  added a recommender to iMeem.
</p>
  <p>
Discussion section -  a wide ranging discussion about music 
recommendations. Some notes: Why is music recommendation different from 
other types? - Very personal nature, music is always related to some 
other context. What is the role of expert?  
</p>
  <h2> Questions from the audience</h2>
Recommendations vs. Search - search engine-based recommenders seem to be
 growing fast - and they are not using a recommender Markus - when apple
 launched genius Gracenotes traffic increased by 10 (Apple uses 
Gracenote's fingerprinting technology).
Hacking&gt; - Elias talks about how Last.fm deals with hacking.  Etienne
 echos the point.
Group Recommenders, Business Models, Discovery vs. Recommendation, 
Trust, shilling.

  <p>
It was a good panel.
</p>
		</div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 16, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_2_the_music.html">Permanent link to this entry</a>
																| <a href="https://kiserai.net/DukeListens/ismir_day_2_the_music#comments.html">Comments [1]</a>
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_1_poster_session">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_1_poster_session.html" title="permalink">ISMIR Day 1 Poster session</a></h3>
		</div>

	<div class="entry-content">
				Between the plenary sessions were two poster sessions - and unlike 
previous ISMIRs - every paper has a poster -- so there's an opportunity 
to talk to every author.  The poster space is spread over two floors - 
As you can see in this photo there's lots of natural light, and the 
space was not too noisy so you could actually hear the conversations.
<p>

<img src="support_files/_IMG_0604.JPG" alt="IMG_0604.JPG" border="0" width="600">
</p><p>

There were many, many posters -  on fascinating topics and I had lots of
 great conversations with the authors.  Here are some highlights (these 
are posters that I found particularly interesting, mainly because they 
overlap some of the work we are doing in our lab)
</p><p>
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_131.pdf">Uncovering Affinity of Artists to Multiple Genres From Social Behaviour Data</a></h2> by 
Claudio Baccigalupo, Justin Donaldson and Enric Plaza. <p> The folks 
from strands have made some interesting data available that is worth 
downloading.  They also have developed a pretty interesting artist 
similarity metric that is extremely easy to collect and seems like it 
gives very good similarities.
</p><p>

<img src="support_files/_IMG_0607.JPG" alt="IMG_0607.JPG" border="0" width="600">
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_267.pdf"> The Quest for Musical Genres: Do the Experts and the Wisdom of Crowds Agree?</a></h2> by
Mohamed Sordo, Oscar Celma, Martin Blech and Enric Guaus 
<p>
I find this analysis to be fascinating - Mohamed and Oscar are trying to
 determine how expert-based annotation (such as you find at  a site like
 MP3.com) compares with social, wisdom-of-the-crowds style annotation.  I
 hope they continue this work to see if social methods have higher 
agreement than independent experts. 
</p><p>
<img src="support_files/_IMG_0608.JPG" alt="IMG_0608.JPG" border="0" width="600">

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_128.pdf">Five Approaches to Collecting Tags for Music</a></h2>
Douglas Turnbull, Luke Barrington and Gert Lanckriet<p>
</p><p>
The poster from the UCSD folks was a real crowd pleaser - not only did 
the poster look great (as if it were professionally designed), the 
poster stimulated lots of conversations and debate.  At the end of day, 
Doug could barely talk his voice was so hoarse.

<img src="support_files/_IMG_0610.JPG" alt="IMG_0610.JPG" border="0" width="600">

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_250.pdf">Collective Annotation of Music From Multiple Semantic Categories</a></h2> by
Zhiyao Duan, Lie Lu and Changshui Zhang
<p>
This was some really interesting work from Microsoft Research Asia on autotagging. 
<img src="support_files/_IMG_0613.JPG" alt="IMG_0613.JPG" border="0" width="600">
</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_111.pdf">Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics
</a></h2>by Hiromasa Fujihara, Masataka Goto and Jun Ogata 
 <p>
Hiromasa is doing some really interesting work with lyrics. His system 
will try to find related lyrics in songs using audio.  This requires 
separating the voice from the rest of the music, using an acoustic model
 of the voice appropriate for singers.  He is currently getting about 
30% accuracy which is a very good result considering how difficult the 
problem is.  
</p><p>
Also, Masataka is a very good professor.  Many times I saw him working the room to bring people to his students' posters.
</p><p>
<img src="support_files/_IMG_0615.JPG" alt="IMG_0615.JPG" border="0" width="600">

</p><h2><a href="http://ismir2008.ismir.net/papers/ISMIR2008_211.pdf">Oh Oh Oh Whoah! Towards Automatic Topic Detection in Song Lyrics</a></h2>
Florian Kleedorfer, Peter Knees and Tim Pohle 
<p>
Florian's research on the clustering lyrics for topic detection is 
really interesting.  Be sure to ask him to show you his demo. (And 
Florian likes to close his eyes when he sings, just like Hugh Grant in <a href="https://www.imdb.com/title/tt0276751/">About a Boy</a>)
</p><p>
<img src="support_files/_IMG_0614.JPG" alt="IMG_0614.JPG" border="0" width="600">
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 16, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_1_poster_session.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>

</div>
            <div class="day">

	<div class="day-title">
	<h2>Monday Sep 15, 2008</h2>
	</div>

	<div class="entry" id="ismir_day_1_plenary_session1">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_1_plenary_session1.html" title="permalink">ISMIR Day 1: Plenary Session #2</a></h3>
		</div>

	<div class="entry-content">
				<b> ISMIR Day 1 - Plenary Session 2 -- Music Recommendation and Recognition</b>
<p>
<a href="http://ismir2008.ismir.net/papers/ISMIR2008_157.pdf">A 
Comparison of Signal-Based Music Recommendation to Genre Labels, 
Collaborative Filtering, Musicological Analysis, Human Recommendation, 
and Random Baseline</a> by Terence Magno and Carl Sable
<br>
Terence describes 3 common recommendation strategies: Genre taxonomies, 
Musicological analysis, Collaborative Filtering and the pitfalls of 
these strategies (no surprises to regular readers of this blog).  
Terence goes on to describe their research with music similarity based 
upon content analysis.   They are using stock MIR techniques (MFCCs, 
GMMs, and similarity measures  like euclidean distance, EMD, etc).  
There goal is to tune the music similarity to match a human notion of 
similarity.
</p><p>
The two authors rated the similarity of 200 song pairs subjectively on a
 1-4 scale.  Then they tuned their similar model maximize the 
correlation with their subjective scale. This training set seems very 
small - using the opinions of only two researchers does not seem likely 
to be able to generate a general dataset useful as a ground truth for 
similarity.
</p><p>
They then conducted a small human evaluation (13 participants) and 
compared their result with AllMusic, Pandora, Last.fm and Random. This 
table shows the ranking (lower is better).

<img src="support_files/_magno.png" alt="magno.png" border="0" height="279" width="487">
</p><p>
I wanted to ask a question about how the evaluation was conducted.  In 
particular I think it is important to note if the users were familiar 
with the recommended tracks or not, and whether or not during the 
evaluation could the participants listen to the music. One would expect 
that a good recommendation for a novel song would not be known by the 
participant - if they can't listen to the unknown recommendation they 
may rate the recommendation poorly.
</p><p>
The results don't agree with other comparisons I've seen between 
content-based and other recommendations strategy. I'm curious as to why.
 In my experience, content-based systems will perform significantly 
worse than these other systems when popular music is used.
</p><p>
<em><a href="http://ismir2008.ismir.net/papers/ISMIR2008_167.pdf">Development of An Automatic Music Selection System Based on Runner’s Step Frequency</a> </em>by
Masahiro Niitsuma, Hiroshi Takaesu, Hazuki Demachi, Masaki Oono and Hiroaki Saito
<br>
This paper presents an automated music selection system based upon a 
runner's step frequency.
They describe a preliminary study that concluded that people tend to 
feel comfortable with the BPM of a song matches their steps-per-minute 
when they are running.
</p><p>
Next they describe their system - first they estimate the running speed 
by looking at accelerometer data amd calculating the Steps per Minute 
from that data.  Next they group music based on its BPM.  Songs with 
similar BPMs (within 4 BPM) are clustered - then  they query the DB with
 runner's SPM and select one track from the corresponding group.
</p><p>
They evaluated the accuracy of the system and the user experience.  The 
user experience was improved when the step tracking system was used.
</p><p>
<img src="support_files/_niitsuma.png" alt="niitsuma.png" border="0" height="351" width="477">

<em><a href="http://ismir2008.ismir.net/papers/ISMIR2008_199.pdf">A Robot Singer With Music Recognition Based on Real-Time Beat Tracking</a></em>

by Kazumasa Murata, Kazuhiro Nakadai, Kazuyoshi Yoshii, Ryu Takeda, 
Toyotaka Torii, Hiroshi G. Okuno, Yuji Hasegawa and Hiroshi Tsujino
</p><p>
Their goal is construct a robot singer that can detect musical beats by 
using its own ears and sing /scat/step according to the detected beat in
 real-time.
</p><p>
Challenges: robust to noise, adaptive to tempo changes, works in realtime.
</p><p> The author went on to describe how the system works and showed 
how their new beat tracker was able to more quickly identify tempo 
changes than previous systems.
</p><p>
The author showed a couple of videos showing a dancing robot that would 
adapt to songs with different tempos. Interesting and fun work. 
</p><p>
<img src="support_files/_robotsinging.png" alt="robotsinging.png" border="0" height="357" width="680">

</p><p>
All in all a good session -- but it was a bit of an odd coupling of one paper on recommendation and two on music recognition.
		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 15, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_1_plenary_session1.html">Permanent link to this entry</a>
																| Comments [0]
								| Comments have been disabled.
						</p>
	</div>

		</div>
	<div class="entry" id="ismir_day_1_plenary_session">

	<div class="entry-title entry-info">
			<h3><a href="ismir_day_1_plenary_session.html" title="permalink">ISMIR Day 1:  Plenary Session</a></h3>
		</div>

	<div class="entry-content">
				<b> Plenary Session 1: User Interfaces and Visualizations</b>
<i> Chair - Roger Dannenberg of CMU</i>
<p> 
<b> Presentation 1</b></p><p>
<em><a href="http://ismir2008.ismir.net/papers/ISMIR2008_166.pdf">Music Thumbnailer: Visualizing Musical Pieces in Thumbnail Images Based on Acoustic Features</a></em> by Kazuyoshi Yoshii and Masataka Goto.
</p><p>
Presenter: Kazuyoshi Yoshii
</p><p>
This paper presents a principled method called MusicThumbnailer to 
transform musical pieces into visual thumbnail images based on acoustic 
features extracted from their audio signals.The goal is to make it easy 
for users to quickly grasp music contents using visual thumbnailing. The
 thumbnail is related to music content. Some examples:
</p><p>
<img src="support_files/_thumbnailing.png" alt="thumbnailing.1.png" border="0" width="600">

Some characteristics to make good thumbnails:
</p><ul>
<li>Try to maximize 'memorability' - large 'smoothness' helps.  
</li><li>Large dynamic range
</li><li> Distinguishability - should be wide variety between various thumbnails 
</li></ul>
The thumbnailing seemed to really be useful for identifying similar 
music although I'm not 100% convinced that these thumbnails are 
memorable - I hope to get to try a few more during the poster session.
<p>
<b> Presentation 2</b></p><p>
<b><a href="http://ismir2008.ismir.net/papers/ISMIR2008_190.pdf">MCIpa: A Music Content Information Player and Annotator for Discovering Music</a></b><br>
Geoffroy Peeters, David Fenech and Xavier Rodet
</p><p>
<em>Speaker: Geoffroy Peeters</em>
</p><p> A simple music database browser that allows you to search a 
large music database.  A very feature-filled music track browser - the 
track browser presents graphically a track - it allows you to browse and
 search within a track. The browser includes a number of visualizations 
including: a scrolling piano roll.  (this visualization seemed too dense
 to be of too much use).  There's also a music structure browser that 
easily lets you find similar segments in a track.  There's a chord 
progression visualization.   They use automatic chord estimation - the 
visualizer uses folder tabs to show the current chords. This was pretty 
neat.  There's also a beat visualizer that shows the downbeat. There's a
 multi-pitch visualization - this has a much more useful piano roll.  
There were lots of other things you can do with the track browser.  You 
can annotate a track, add markers, labels etc.  This tool is developed 
for professional music annotators.  The track browser seems to be really
 useful.  I'm glad I'll be able to have a closer look during the poster 
sessions.
<a href="http://support_files/_peeters.png"><img src="support_files/_peeters.png" alt="peeters.png" border="0" width="600"></a>
</p><p>
<b> Presentation 3</b></p><p>
<em><a href="http://ismir2008.ismir.net/papers/ISMIR2008_123.pdf">Development of a Music Organizer for Children</a></em><br>Edmond Zhang and Sally Jo Cunningham
</p><p>
Sally Jo is a great speaker - she presented a strong justification about
 why we should care about building a music organizer for children with 
some funny stories (kids say the darndest things). She talked about the 
challenges in the participatory design process with children as 
participants. Some observations:  Current music player has "too much 
stuff". Adult player "too boring" - players designed for kids "too 
childish".  Paper prototypes are useless with kids.  They had to use 
high-fidelity prototypes - which means fewer iterations, but this led to
 better feedback.  They solicted reviews from design experts.
</p><p>
Sally Jo showed some of the hi-fi prototypes.  Kids really like to rate 
songs. They also have a parent interface made to look "as boring as 
possible" to discourage the kids from using it - where the parents can 
manage things like files and directories. Kids can give each other 
gifts.
</p><p>
Lessons - kids want less text, using the system is kind of like a game -
 the designers must make sure that actions are obvious and undoable.  
Skins important. Kids liked the games - more so than just organize the 
music.  Next steps: get it running the the OLPC.  - A great talk.
</p><p>
<img src="support_files/_sallyjo.png" alt="sallyjo.png" border="0" height="339" width="414">



		</p></div>
	
	<div class="entry-footer entry-info">
	<p class="entry-date">Posted on: Sep 15, 2008</p>
	<p class="entry-author">Posted by: plamere</p>
	<p class="entry-category">Category: General</p>
	<p class="entry-links">
		<a href="ismir_day_1_plenary_session.html">Permanent link to this entry</a>
																| <a href="https://kiserai.net/DukeListens/ismir_day_1_plenary_session#comments.html">Comments [1]</a>
								| Comments have been disabled.
						</p>
	</div>

		</div>

</div>
    
<!-- End SiteCatalyst code version: G.5. -->

<div id="boilerplate">
<div id="innerboilerplate">
<p>This blog copyright 2010 by plamere</p>
</div>
</div>

</div><!-- end #innercontent -->
</div><!-- end #content -->

           <div id="sidebars">
<div id="innersidebars">

<div class="sidebar sidebar-a" id="nav">
<div class="innersidebar">



<!-- about me -->

<div class="aboutme sidebar-sect default-expanded">
<div class="sidebar-sect-title">
<h2>About this weblog</h2>
</div>
<div class="sidebar-sect-content">
  <!-- weblog about setting -->
<p><i>(This is an archive of most of Paul Lamere's weblog from blogs.sun.com/plamere/  <br />
This is not the original.  Some links do not work.  <br />
An <a href="entry_index.html">entry index</a> is available.)</i></p>
  <p>I am a researcher in Sun Labs where I explore new ways to organize, search for, and discover music. Read more on the <a href="http://research.sun.com/projects/dashboard.php?id=153">Search Inside the Music project page </a>.
</p>
</div>
</div>



<!-- paging -->

<div class="paging sidebar-sect default-expanded">
<div class="sidebar-sect-title">
<h2>Your Current Location</h2>
</div>
<div class="sidebar-sect-content">
           
	<p class="location">
			You are browsing the main weblog
		</p>

	<p class="prev-next">
	                            « <a href="page6.html">Previous page</a> |  
                <a href="page0.html">Main</a>
                | <a href="page8.html">Next page</a> »
    	</p>

    </div>
</div>
<h2>What I am listening to</h2>
<object><embed src="support_files/user_profile_embed.swf" type="application/x-shockwave-flash" wmode="transparent" height="200" width="200"></object>
<h2>What I am reading</h2>
<div class="f"><a href="https://www.google.com/reader/shared/user/07268145224739680674/state/com.google/broadcast">Google Reader »</a></div>
<!-- recent entries -->

<div class="recent sidebar-sect default-expanded">
<div class="sidebar-sect-title">
<h2>Up to 10 Recent Entries</h2>
</div>
<div class="sidebar-sect-content">
  <ul>
      <li><a href="last_post.html">Last Post</a></li>
      <li><a href="the_best_job_in_the.html">The best job in the world</a></li>
      <li><a href="music_gone_viral.html">Music gone viral?</a></li>
      <li><a href="music_recommendation_and_the_long.html">Music Recommendation and the Long Tail</a></li>
      <li><a href="sxsw_artist_catalog_updated.html">SXSW Artist Catalog updated</a></li>
      <li><a href="most_overused_animals_of_indie.html">Most overused animals of indie rock?</a></li>
      <li><a href="blip_fm_api.html">Blip.fm API</a></li>
      <li><a href="new_release_of_sphinx_4.html">New release of Sphinx-4</a></li>
      <li><a href="styling_the_sxsw_artist_catalog.html">Styling the SXSW Artist Catalog</a></li>
      <li><a href="getting_the_most_space_on.html">Getting the most space on the marquee</a></li>
    </ul>
</div>
</div>



<!-- feeds not available in this archive, sorry -->
<div class="categories sidebar-sect">
   <div class="sidebar-sect-title">
   <h2>Categories</h2>
   <i>(Original blog had several other categories)</i>
   </div>
   <div class="sidebar-sect-content">
       <ul class="rCategory">
           <li><a href="category-freakomendations-page0.html">freakomendations</a></li>
       </ul>
   </div>
</div>
<!-- weblog menu not available in this archive, sorry -->
<!-- search not available in this archive, sorry -->
</form>
<script type="text/javascript">prepareSearchForm();</script>
</div>
</div>



</div><!-- end .innersidebar -->
</div><!-- end .sidebar-a -->

<div class="sidebar sidebar-b">
<div class="innersidebar">



<!-- bookmarks -->

<div class="links sidebar-sect">
<div class="sidebar-sect-title">
<h2>Bookmarks</h2>
</div>
<div class="sidebar-sect-content">
<ul class="rFolder">
                    <li class="rFolderItem">
                                <a href="http://www.mediaor.com/" title="Aggregation of music blogs" class="rBookmark0">Me*dia*or</a>
                </li>
            <li class="rFolderItem">
                                <a href="http://mobblog.cs.ucl.ac.uk/" title="Research: Trust, Reputation, Recommendations and Mobility" class="rBookmark0">MobBlog</a>
                </li>
            <li class="rFolderItem">
                                <a href="offices_of_music_2_013.html" title="Photos of Music 2.0 Offices" class="rBookmark0">Offices</a>
                </li>
            <li class="rFolderItem">
                                <a href="http://research.sun.com/projects/dashboard.php?id=153" title="The search inside the music project page" class="rBookmark0">SITM</a>
                </li>
            <li class="rFolderItem">
                                <a href="https://savetherobot.wordpress.com/" title="Chris Dahlen's work blog" class="rBookmark0">Save the robot</a>
                </li>
            <li class="rFolderItem">
                                <a href="http://blogs.sun.com/searchguy/" title="" class="rBookmark0">Search Guy</a>
                </li>
            <li class="rFolderItem">
                                <a href="http://thetasteblog.com/" title="An aggregation of recommender blogs" class="rBookmark0">The Taste Blog</a>
                </li>
            <li class="rFolderItem">
                                <a href="http://musicmachinery.com/feed/"><img class="smrssbadge" src="support_files/smrssbadge.gif" alt="URL of site''s RSS feed"></a>
                        <a href="http://musicmachinery.com/" title="a blog about music technology" class="rBookmark10">Music Machinery</a>
                </li>
                    </ul>
</div>
</div>



</div><!-- end .innersidebar -->
</div><!-- end .sidebar-b -->

</div><!-- end #innersidebars -->
</div><!-- end #sidebars -->
    
</div><!-- end #innerpagewrap -->
</div><!-- end #pagewrap -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script src="support_files/ga.js" type="text/javascript"></script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-187070-1");
pageTracker._trackPageview();
</script>
</body></html>
